What should users be allowed to deploy?
ğŸ”’ Principle

Namespace isolation + RBAC + policy enforcement
Users should deploy application-level resources, not cluster-level or infrastructure resources.

âœ… Allowed for users (namespace-scoped)

These are generally safe and expected:

Core workload

Deployment

StatefulSet (if needed)

Job, CronJob

Pod (optional â€“ often blocked in prod to force Deployments)

ReplicaSet (indirectly via Deployment)

Networking

Service

Ingress (prefer via approved IngressClass)

Config & identity

ConfigMap

Secret (prefer external secret stores if possible)

ServiceAccount

Role, RoleBinding (namespace only)

Scaling & reliability

HorizontalPodAutoscaler

PodDisruptionBudget

âŒ Should NOT be allowed to users

These should be platform/team-owned only:

ClusterRole, ClusterRoleBinding

CustomResourceDefinition (CRD)

Namespace

Node, NodePool

StorageClass

IngressClass

MutatingWebhookConfiguration

ValidatingWebhookConfiguration

NetworkPolicy (optional â€“ depends on model)

ğŸ‘‰ Enforce this with:

Azure RBAC for AKS

Kubernetes RBAC

OPA Gatekeeper / Kyverno policies

2ï¸âƒ£ What quotas should be set on a namespace?
ğŸ¯ Goal

Prevent:

Noisy neighbor issues

Accidental cluster exhaustion

One team eating all IPs / CPU / memory

You typically want 3 layers:

ResourceQuota

LimitRange

Optional object count limits

ğŸ“¦ ResourceQuota (baseline example)
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-quota
  namespace: team-a
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    pods: "50"
    services: "20"
    persistentvolumeclaims: "10"


ğŸ“Œ Tune per environment:

Dev: smaller quotas

Prod: higher but controlled

Shared tools: separate namespace

ğŸ“ LimitRange (MANDATORY)

This stops users from creating pods with no limits.

apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: team-a
spec:
  limits:
  - default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    type: Container

ğŸ”¢ Object quotas (optional but useful)
count/deployments.apps: "10"
count/ingresses.networking.k8s.io: "5"


Great for preventing â€œoops I deployed 200 thingsâ€.

3ï¸âƒ£ Standard templates â€“ what must users fill in?
ğŸ¯ Principle

Users should provide intent, not plumbing

Your templates should:

Hide complexity

Force best practices

Be policy-compliant by default

ğŸ“„ Deployment template â€“ REQUIRED user inputs

These are the fields users must fill (everything else should be defaulted):

ğŸ”¹ Mandatory fields

app.name

image.repository

image.tag

containerPort

replicas (or autoscaling config)

resources.requests.cpu

resources.requests.memory

resources.limits.cpu

resources.limits.memory

livenessProbe

readinessProbe

ğŸ”¹ Strongly recommended

env (non-secret)

secretRefs

serviceAccountName

nodeSelector / tolerations (if needed)

topologySpreadConstraints

podDisruptionBudget

ğŸ”¹ Example opinionated Deployment skeleton
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ app.name }}
  labels:
    app: {{ app.name }}
spec:
  replicas: {{ replicas }}
  selector:
    matchLabels:
      app: {{ app.name }}
  template:
    metadata:
      labels:
        app: {{ app.name }}
    spec:
      serviceAccountName: {{ serviceAccount }}
      containers:
      - name: {{ app.name }}
        image: {{ image.repository }}:{{ image.tag }}
        ports:
        - containerPort: {{ containerPort }}
        resources:
          requests:
            cpu: {{ cpuRequest }}
            memory: {{ memoryRequest }}
          limits:
            cpu: {{ cpuLimit }}
            memory: {{ memoryLimit }}
        readinessProbe: {{ readinessProbe }}
        livenessProbe: {{ livenessProbe }}

ğŸ§± Service template â€“ REQUIRED

Users must provide:

service.type (ClusterIP default)

ports

targetPort

Avoid LoadBalancer unless platform-approved.

ğŸ§  Strongly recommended platform controls (donâ€™t skip)
âœ… Enforce via policy (OPA / Kyverno)

Pods must have:

resource requests & limits

non-root user

readOnlyRootFilesystem

No latest image tags

Approved registries only

No privileged containers

Ingress only via approved class

ğŸ·ï¸ Mandatory labels (enforceable)

Require these on every object:

app.kubernetes.io/name
app.kubernetes.io/team
app.kubernetes.io/environment
cost-center
owner


This pays off BIG TIME for:

Cost allocation

Incident response

Cleanup

ğŸ§­ Recommended ownership model
Area	Owner
AKS cluster	Platform team
Node pools	Platform
Ingress controller	Platform
Network policies	Platform
Namespace	Platform
Workloads	App teams
ğŸš€ If you want, next steps I can help with

A full namespace bootstrap template

A reference RBAC model per team

Kyverno / Gatekeeper policy examples

A Golden Path repo structure

Mapping this to Azure RBAC + Entra ID groups

Just tell me how deep you want to go ğŸ™‚

A full namespace bootstrap template

Perfect â€” hereâ€™s a production-grade â€œnamespace bootstrapâ€ template that platform teams commonly use for a central AKS cluster.
This is what you apply once per team/environment to make the namespace ready before any app teams deploy workloads.

Iâ€™ll give you:

What this bootstrap includes

A complete YAML bundle

Notes on what to customize vs lock down

1ï¸âƒ£ What a namespace bootstrap should include

At minimum, a â€œreadyâ€ namespace should have:

âœ… Namespace + standard labels
âœ… ResourceQuota
âœ… LimitRange
âœ… Default ServiceAccount (locked down)
âœ… RBAC for app teams (namespace-scoped)
âœ… NetworkPolicy (baseline deny / allow)
âœ… Pod Security enforcement
âœ… Optional: image pull secret, PDB defaults

2ï¸âƒ£ Full Namespace Bootstrap Template (YAML)

You can keep this as a single file or split it into kustomize/helm.

ğŸŸ¦ Namespace
apiVersion: v1
kind: Namespace
metadata:
  name: team-a-dev
  labels:
    name: team-a-dev
    environment: dev
    team: team-a
    app.kubernetes.io/managed-by: platform
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted

ğŸ“¦ ResourceQuota
apiVersion: v1
kind: ResourceQuota
metadata:
  name: team-a-quota
  namespace: team-a-dev
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    pods: "50"
    services: "20"
    persistentvolumeclaims: "10"

ğŸ“ LimitRange (MANDATORY)
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: team-a-dev
spec:
  limits:
  - type: Container
    default:
      cpu: "500m"
      memory: "512Mi"
    defaultRequest:
      cpu: "250m"
      memory: "256Mi"
    max:
      cpu: "2"
      memory: "2Gi"

ğŸ” Default ServiceAccount (locked down)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: default
  namespace: team-a-dev
automountServiceAccountToken: false


App teams should create their own ServiceAccounts when needed.

ğŸ‘¤ Application ServiceAccount (example)
apiVersion: v1
kind: ServiceAccount
metadata:
  name: app-sa
  namespace: team-a-dev

ğŸ”‘ RBAC â€“ Namespace Admin (Team Leads)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: namespace-admin
  namespace: team-a-dev
rules:
- apiGroups: ["", "apps", "batch", "autoscaling", "networking.k8s.io"]
  resources: ["*"]
  verbs: ["*"]

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: namespace-admin-binding
  namespace: team-a-dev
subjects:
- kind: Group
  name: aks-team-a-admins   # Entra ID group
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: namespace-admin
  apiGroup: rbac.authorization.k8s.io

ğŸ” RBAC â€“ Application Developers (Restricted)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: app-developer
  namespace: team-a-dev
rules:
- apiGroups: ["", "apps", "batch", "autoscaling"]
  resources:
    - deployments
    - statefulsets
    - daemonsets
    - jobs
    - cronjobs
    - services
    - configmaps
    - secrets
    - pods
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: app-developer-binding
  namespace: team-a-dev
subjects:
- kind: Group
  name: aks-team-a-devs   # Entra ID group
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: app-developer
  apiGroup: rbac.authorization.k8s.io

ğŸŒ NetworkPolicy â€“ Default Deny (baseline)
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: team-a-dev
spec:
  podSelector: {}
  policyTypes:
  - Ingress
  - Egress

ğŸŒ NetworkPolicy â€“ Allow same-namespace traffic
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-namespace-internal
  namespace: team-a-dev
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}

ğŸŒ NetworkPolicy â€“ Allow DNS
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: team-a-dev
spec:
  podSelector: {}
  egress:
  - to:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: kube-system
    ports:
    - protocol: UDP
      port: 53

ğŸ·ï¸ (Optional) Enforced Labels via Policy (example intent)

Implement with Kyverno or Gatekeeper, not raw YAML

Required labels:

app.kubernetes.io/name

app.kubernetes.io/team

app.kubernetes.io/environment

owner

cost-center

3ï¸âƒ£ What app teams should NEVER touch

ğŸš« Namespace
ğŸš« ResourceQuota / LimitRange
ğŸš« RBAC
ğŸš« NetworkPolicy (unless delegated)
ğŸš« Pod Security settings

These stay platform-owned.

4ï¸âƒ£ How this fits into a Golden Path

Typical flow:

Platform runs namespace bootstrap

App team gets access via Entra ID group

App team deploys using approved Helm/Kustomize templates

Policies enforce guardrails automatically

Want next?

I can:

Convert this into Helm or Kustomize

Add Kyverno policies that enforce your standards

Create a per-environment version (dev/test/prod)

Map this fully to Azure RBAC for AKS

Just tell me ğŸ‘

A Golden Path repo structure

Nice, youâ€™re clearly building this the right way ğŸ‘
A Golden Path repo should make the right thing the easy thing for app teams, while keeping platform concerns cleanly separated.

Below is a proven repo structure that works very well for central AKS + multiple teams, followed by how each folder is used and how teams actually consume it.

ğŸ—ï¸ Golden Path Repository Structure
aks-golden-path/
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ onboarding.md
â”‚   â”œâ”€â”€ standards.md
â”‚   â”œâ”€â”€ security.md
â”‚   â”œâ”€â”€ networking.md
â”‚   â””â”€â”€ troubleshooting.md
â”‚
â”œâ”€â”€ platform/
â”‚   â”œâ”€â”€ namespaces/
â”‚   â”‚   â”œâ”€â”€ team-a/
â”‚   â”‚   â”‚   â”œâ”€â”€ dev/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ namespace.yaml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ resourcequota.yaml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ limitrange.yaml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ rbac.yaml
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ networkpolicy.yaml
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ kustomization.yaml
â”‚   â”‚   â”‚   â””â”€â”€ prod/
â”‚   â”‚   â””â”€â”€ team-b/
â”‚   â”‚
â”‚   â”œâ”€â”€ cluster/
â”‚   â”‚   â”œâ”€â”€ ingress/
â”‚   â”‚   â”‚   â”œâ”€â”€ nginx/
â”‚   â”‚   â”‚   â””â”€â”€ appgw/
â”‚   â”‚   â”œâ”€â”€ policies/
â”‚   â”‚   â”‚   â”œâ”€â”€ kyverno/
â”‚   â”‚   â”‚   â””â”€â”€ gatekeeper/
â”‚   â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ logging/
â”‚   â”‚   â””â”€â”€ autoscaling/
â”‚   â”‚
â”‚   â””â”€â”€ tooling/
â”‚       â”œâ”€â”€ cert-manager/
â”‚       â”œâ”€â”€ external-dns/
â”‚       â”œâ”€â”€ external-secrets/
â”‚       â””â”€â”€ metrics-server/
â”‚
â”œâ”€â”€ templates/
â”‚   â”œâ”€â”€ helm/
â”‚   â”‚   â”œâ”€â”€ app-base/
â”‚   â”‚   â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ values.yaml
â”‚   â”‚   â”‚   â””â”€â”€ templates/
â”‚   â”‚   â”‚       â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”‚       â”œâ”€â”€ service.yaml
â”‚   â”‚   â”‚       â”œâ”€â”€ ingress.yaml
â”‚   â”‚   â”‚       â”œâ”€â”€ hpa.yaml
â”‚   â”‚   â”‚       â””â”€â”€ pdb.yaml
â”‚   â”‚   â””â”€â”€ job/
â”‚   â”‚
â”‚   â””â”€â”€ kustomize/
â”‚       â”œâ”€â”€ base/
â”‚       â”‚   â”œâ”€â”€ deployment.yaml
â”‚       â”‚   â”œâ”€â”€ service.yaml
â”‚       â”‚   â””â”€â”€ kustomization.yaml
â”‚       â””â”€â”€ overlays/
â”‚           â”œâ”€â”€ dev/
â”‚           â”œâ”€â”€ test/
â”‚           â””â”€â”€ prod/
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ simple-api/
â”‚   â”œâ”€â”€ worker/
â”‚   â””â”€â”€ cronjob/
â”‚
â”œâ”€â”€ ci/
â”‚   â”œâ”€â”€ github-actions/
â”‚   â”‚   â”œâ”€â”€ validate.yaml
â”‚   â”‚   â””â”€â”€ deploy.yaml
â”‚   â””â”€â”€ azure-devops/
â”‚
â””â”€â”€ scripts/
    â”œâ”€â”€ bootstrap-namespace.sh
    â”œâ”€â”€ validate-manifests.sh
    â””â”€â”€ render-helm.sh

ğŸ”‘ How each part is used
ğŸ“ platform/ (Platform-owned ONLY)

Not writable by app teams

platform/namespaces/

One folder per team per environment

Contains your namespace bootstrap

Applied by platform CI/CD

ğŸ‘‰ This is where your previous namespace template lives.

platform/cluster/

Cluster-wide components:

Ingress controllers

Kyverno / Gatekeeper policies

Monitoring & logging

Autoscalers

ğŸš« App teams never touch this.

platform/tooling/

Shared add-ons:

cert-manager

external-secrets

external-dns

metrics-server

ğŸ“ templates/ (The Golden Path â¤ï¸)

This is what app teams actually use.

templates/helm/app-base/

Your opinionated application chart:

Security defaults baked in

Resource limits required

Probes mandatory

Labels enforced

Ingress standardized

Users only edit values.yaml.

Typical required fields:

app:
  name:
  team:
  environment:

image:
  repository:
  tag:

resources:
  requests:
  limits:

templates/kustomize/

For teams that donâ€™t want Helm:

base/ â†’ locked down

overlays/ â†’ env-specific tweaks

ğŸ“ examples/

Copy-paste ready references:

Simple REST API

Background worker

CronJob

ğŸ’¡ These dramatically reduce onboarding friction.

ğŸ“ docs/

Short, opinionated docs:

â€œHow to deploy your first app in 10 minutesâ€

â€œWhat youâ€™re allowed to doâ€

â€œCommon mistakesâ€

Keep docs short and prescriptive.

ğŸ“ ci/

Shared pipelines:

Manifest validation (OPA/Kyverno, kubeconform)

Helm linting

Security scanning

App teams can reuse these verbatim.

ğŸ“ scripts/

Small helpers:

Namespace bootstrap

Local validation

Rendering manifests

ğŸ”„ How app teams actually consume this
Option A â€“ Reference as a dependency

App repo:

dependencies:
- name: app-base
  version: 1.2.0
  repository: https://internal-repo/charts

Option B â€“ Template generator
./scripts/create-app.sh my-service

Option C â€“ Fork (least preferred)

Allowed, but loses central upgrades.

ğŸ§  Governance model (important)
Item	Control
Namespace	Platform
Quotas	Platform
Policies	Platform
Templates	Platform
values.yaml	App teams
CI pipelines	Shared
ğŸš€ What makes this a real Golden Path

âœ” â€œHello Worldâ€ deployable in <10 min
âœ” Impossible to deploy insecure workloads
âœ” No YAML copy-paste from old clusters
âœ” Upgrades centrally managed
âœ” Clear ownership boundaries
